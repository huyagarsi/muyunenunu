<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>How do I compute multiple per-sample gradients efficiently? | PulseVibe</title><meta name=generator content="Hugo 0.98.0"><meta name=description content="I am trying to compute multiple loss gradients efficiently (without a for loop) in PyTorch. Given:
import torch from torch import nn class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.linear = nn.Sequential( nn.Linear(input_size, 16, bias=False), nn.Linear(16, output_size, bias=False), ) def forward(self, x): return self.linear(x) device = &#34;cpu&#34; input_size = 2 output_size = 2 x = torch.randn(10, 1, input_size).to(device) y = torch.randn(10, 1, output_size).to(device) model = NeuralNetwork().to(device) loss_fn = nn.MSELoss() def loss_grad(x, label): y = model(x) loss = loss_fn(y, label) grads = torch."><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cayman/css/normalize.css><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel=stylesheet type=text/css><link rel=stylesheet href=https://assets.cdnweb.info/hugo/cayman/css/cayman.css><link rel=apple-touch-icon sizes=180x180 href=./apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=./favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=./favicon-16x16.png><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css integrity=sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js integrity=sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body><section class=page-header><h1 class=project-name>PulseVibe</h1><h2 class=project-tagline></h2><nav><a href=./index.html class=btn>Blog</a>
<a href=./sitemap.xml class=btn>Sitemap</a>
<a href=./index.xml class=btn>RSS</a></nav></section><section class=main-content><h1>How do I compute multiple per-sample gradients efficiently?</h1><div><strong>Publish date: </strong>2024-08-16</div><img src=https://cdn.statically.io/img/cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><p>I am trying to compute multiple loss gradients efficiently (without a for loop) in PyTorch. Given:</p><pre class="lang-py prettyprint-override"><code>import torch from torch import nn class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.linear = nn.Sequential( nn.Linear(input_size, 16, bias=False), nn.Linear(16, output_size, bias=False), ) def forward(self, x): return self.linear(x) device = "cpu" input_size = 2 output_size = 2 x = torch.randn(10, 1, input_size).to(device) y = torch.randn(10, 1, output_size).to(device) model = NeuralNetwork().to(device) loss_fn = nn.MSELoss() def loss_grad(x, label): y = model(x) loss = loss_fn(y, label) grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True) return grads </code></pre><p>The following works, but uses a for loop:</p><pre><code># inefficient but works def compute_for(): grads = [loss_grad(x[i], y[i]) for i in range(x.shape[0])] print(grads) compute_for() </code></pre><p>For efficiency, I tried using <code>torch.vmap</code> instead:</p><pre><code># potentially more efficient but doesn't work def compute_vmap(): grads = torch.vmap(loss_grad)(x, y) print(grads) compute_vmap() </code></pre><p>I was expecting it to compute the gradients of the losses w.r.t. the parameters for each element in <code>x, y</code>. Instead, I get an error:</p><pre><code>RuntimeError: element 0 of tensors does not require grad </code></pre><p>As I understand, this means that elements from the tensor <code>x</code> will be computed and they don't individually require grad.</p><p>How can I modify this code so that it computes all gradients? Or is there another method to do that?</p><span class=d-none itemprop=commentCount>3</span><h2 class=mb0 data-answercount=1>1 Answer</h2><p>The per-sample gradients may be computed using <code>vmap</code> as shown in the relevant <a href=# rel="nofollow noreferrer">tutorial</a>:</p><pre><code>from torch.func import functional_call, vmap, grad def compute_loss(params, buffers, sample, target): batch = sample.unsqueeze(0) targets = target.unsqueeze(0) predictions = functional_call(model, (params, buffers), (batch,)) loss = loss_fn(predictions, targets) return loss params = {k: v.detach() for k, v in model.named_parameters()} buffers = {k: v.detach() for k, v in model.named_buffers()} ft_compute_grad = grad(compute_loss) ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0)) ft_per_sample_grads = ft_compute_sample_grad(params, buffers, x, y) print(ft_per_sample_grads) </code></pre><p>These match the gradients computed individually for each pair <code>(x[i], y[i])</code>.</p><span class=d-none itemprop=commentCount></span><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmirpJawrLvVnqmfpJ%2Bse6S7zGiorp2jqbawutJobm9rZWiCdYKOoaawZZSkeqp5wqikqa2kmnquwcutoKmklWK9pr6MrJimqJyaeqi%2BwJ2gnqakqHqmssWimqKdnqm5ug%3D%3D</p><footer class=site-footer><span class=site-footer-credits>Made with <a href=https://gohugo.io/>Hugo</a>. Â© 2022. All rights reserved.</span></footer></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>